<!DOCTYPE html>

<HTML>
<HEAD>
  <META content="IE=5.0000" http-equiv="X-UA-Compatible">
  <META name="description" content="Zhonqin Wang's home page"> 
  <META http-equiv="Content-Type" content="text/html; charset=gb2312">
  <LINK href="files/doc.css" 
    rel="stylesheet" type="text/css"> 
  <TITLE>Zhonqin Wang</TITLE> 
  <META name="GENERATOR" content="MSHTML 11.00.10570.1001">
  <TITLE>line-height</TITLE> 
  <style>
    .little-line-height {line-height: 0.5;}
    .normal-line-height {line-height: 1.5;}
    .large-line-height {line-height: 2;}
  </style>
  <title>一半左对齐一半右对齐</title>
  <style>
      .container {
          display: flex;
          justify-content: space-between;
      }
      .left {
          text-align: left;
          width: 50%;
      }
      .right {
          text-align: right;
          width: 50%;
      }
  </style>
</HEAD>


<BODY> 
  <DIV id="layout-content" style="margin-top: 25px;">
  <TABLE>
    <TBODY>
    <TR>
      <TD width="670">
        <DIV id="toptitle">
        <H1>Dr. Zhonqin Wang &nbsp;</H1></DIV>
        <BR>
        <BR>
        <BR> Email:  
        <A class="normal-line-height" href="mailto:zhongqin.wang@uts.edu.au"> zhongqin.wang@uts.edu.au</A>; 
        <BR> Phone: 
        <A class="normal-line-height" href="+61 0416167758"> +61 0416167758</A>;
        <BR> Github: 
        <A class="normal-line-height" href="https://github.com/Zhongqin-Wang">https://github.com/Zhongqin-Wang</A>;
        <BR> Google scholar:
        <A class="normal-line-height" href="https://scholar.google.com/citations?user=F9UMFwIAAAAJ&hl=en">https://scholar.google.com</A>
        <BR><BR></P>
      </TD>
      <TD>
        <IMG width="150" src="files/person_photo.jpg" border="0">
      </TD>
    </TR>
    <TR></TR></TBODY>
  </TABLE>
  <DIV id="layout-content" style="margin-top: 25px;">


  <H2>RESEARCH INTERESTS</H2>
  <P class="normal-line-height"> I specialize in designing Contactless, Sensorless, and Wireless sensing solutions beyond vision, aiming to 
    build commercialized sensing applications for a smart and privacy-protected life. My current research interests 
    include:
  </P>

  <ul>
    <li class="little-line-height">Wireless Sensing and Communication</li>
    <li class="little-line-height">Contactless Healthcare and Eldercare</li>
    <li class="little-line-height">Radio-Visual Joint Learning and Sensing</li>
    <li class="little-line-height">Contactless Healthcare and Eldercare</li>
  </ul>

  <H2>WORK EXPERIENCE</H2>
  <div class="container">
    <div class="left"><b>University of Technology Sydney, Australia</b></div>
    <div class="right">Feb. 2024 - Present</div>
  </div>
  <p><i>Research Fellow supervised by Prof. Andrew Zhang</i></p>
  <ul>
    <li>Researching</li>
    <li>Master/PhD student supervision</li>
  </ul>
  <H2>Publications</H2>
    <table class="pub_table">
    <!-- <tbody> -->

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/kosmos2.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;<u>Zhiliang Peng</u>, &ast;Wenhui Wang, &ast;Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei
          <br><b>Kosmos-2: Grounding Multimodal Large Language Models to the World</b>
          <br>
          [<a href="https://arxiv.org/abs/2306.14824">Paper</a>]
          [<a href="https://aka.ms/kosmos-2-demo">Demo</a>]
          [<a href="https://github.com/microsoft/unilm/tree/master/kosmos-2">Code</a>]
          <img src="https://img.shields.io/github/stars/microsoft/unilm?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/g2sd.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;Wei Huang, &ast;<u>Zhiliang Peng</u>, Li Dong, Furu Wei, Jianbin Jiao, Qixiang Ye
          <br><b>Generic-to-Specific Distillation of Masked Autoencoders</b>
          <br>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023
          <br>
          [<a href="https://arxiv.org/abs/2302.14771">Paper</a>]
          [<a href="https://github.com/pengzhiliang/G2SD">Code</a>]
          <img src="https://img.shields.io/github/stars/pengzhiliang/G2SD?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/Conformer_tpami.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Zonghao Guo, Wei Huang, Yaowei Wang, Lingxi Xie, Jianbin Jiao, Qixiang Ye
          <br><b>Conformer: Local features coupling global representations for visual recognition and detection</b>
          <br>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023
          <br>
          [<a href="https://doi.org/10.1109/TPAMI.2023.3243048">Paper</a>]
          [<a href="https://github.com/pengzhiliang/Conformer">Code</a>]
          <img src="https://img.shields.io/github/stars/pengzhiliang/Conformer?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/maskdistill.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei
          <br><b>A Unified View of Masked Image Modeling</b>
          <br> Transactions on Machine Learning Research, 2023
          <br>
          [<a href="https://openreview.net/pdf?id=wmGlMhaBe0">Paper</a>]
          [<a href="https://github.com/microsoft/unilm/blob/master/unimim">Code</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/magneto.png" class="papericon"></td>
        <td 
          class="pub_td2">Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, <u>Zhiliang Peng</u>, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, Furu Wei
          <br><b>Foundation Transformers</b>
          <br>International Conference on Machine Learning, 2023
          <br>
          [<a href="https://arxiv.org/abs/2210.06423">arXiv preprint</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/beit3.png" class="papericon"></td>
        <td 
          class="pub_td2">Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, <u>Zhiliang Peng</u>, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei
          <br><b>Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</b>
          <br>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023
          <br>
          [<a href="https://arxiv.org/abs/2208.10442">Paper</a>]
          [<a href="https://github.com/microsoft/unilm/blob/master/beit3">Code</a>]
          <img src="https://img.shields.io/github/stars/microsoft/unilm?style=social">
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/beit2.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei
          <br><b>BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers</b>
          <br>
          [<a href="https://arxiv.org/abs/2208.06366">arXiv preprint</a>]
          [<a href="http://aka.ms/beit2">Code</a>]
          <img src="https://img.shields.io/github/stars/microsoft/unilm?style=social">
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/imTED.png" class="papericon"></td>
        <td 
          class="pub_td2">Xiaosong Zhang, Feng Liu, <u>Zhiliang Peng</u>, Zonghao Guo, Fang Wan, Xiangyang Ji, Qixiang Ye
          <br><b>Integral Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</b>
          <br>
          [<a href="https://arxiv.org/abs/2205.09613">arXiv preprint</a>]
        </td>
      </tr>


      <tr>
        <td class="pub_td1"><img src="files/PaperFig/Conformer.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, Qixiang Ye
          <br><b>Conformer: Local features coupling global representations for visual recognition</b>
          <br>Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021
          <br>
          [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Peng_Conformer_Local_Features_Coupling_Global_Representations_for_Visual_Recognition_ICCV_2021_paper.html">Paper</a>]
          [<a href="https://github.com/pengzhiliang/Conformer">Code</a>]
          <img src="https://img.shields.io/github/stars/pengzhiliang/Conformer?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/tscam.png" class="papericon"></td>
        <td 
          class="pub_td2">Wei Gao, Fang Wan, Xingjia Pan, <u>Zhiliang Peng</u>, Qi Tian, Zhenjun Han, Bolei Zhou, Qixiang Ye
          <br><b>TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization</b>
          <br>Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021
          <br>
          [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Gao_TS-CAM_Token_Semantic_Coupled_Attention_Map_for_Weakly_Supervised_Object_ICCV_2021_paper.html">Paper</a>]
          [<a href="https://github.com/vasgaowei/TS-CAM">Code</a>]
          <img src="https://img.shields.io/github/stars/vasgaowei/TS-CAM?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/LDA.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Wei Huang, Zonghao Guo, Xiaosong Zhang, Jianbin Jiao, Qixiang Ye
          <br><b>Long-tailed Distribution Adaptation</b>
          <br>Proceedings of the 29th ACM International Conference on Multimedia, 2021
          <br>
          <!-- [<a href="https://dl.acm.org/doi/10.1145/3474085.3475479">Paper</a>] -->
          [<a href="https://arxiv.org/abs/2110.02686">Paper</a>]
          [<a href="https://github.com/pengzhiliang/LDA">Code</a>]
          <br>
        </td>
      </tr>
    <!-- </tbody> -->
    </table>

    <!-- <br>
    <H2>Awards</H2>
        <LI>	Excellent Student Scholarship, Chinese Academy of Sciences, 2020.  </LI> -->
  <H2>Github Statistics</Source></H2>
      <td class="pub_td1"><img src="https://github-readme-stats.vercel.app/api?username=pengzhiliang&show_icons=true&include_all_commits=true&title_color=2c86ea&icon_color=2c86ea&text_color=00c800&bg_color=00000000"></td>
    
  
  <br> <br> 
  <H2>Statistics</H2>
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5063gq35g0n&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>

</BODY>
</HTML>
